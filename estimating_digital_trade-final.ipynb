{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4466b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT LIBRARIES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from statsmodels.genmod.families import Binomial\n",
    "from scipy.io import loadmat\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold, PredefinedSplit, ParameterGrid\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.optimize import linprog\n",
    "from scipy.sparse import kron, eye, vstack, csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD THE DATA, CLEAN, AND SET THE CROSS-VALIDATION / PREDICTION DATASET\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('data.csv', index_col=0)\n",
    "data['category'] = data['category'].replace('online-accommodation', 'online-travel-market')\n",
    "\n",
    "\n",
    "# Set distance to 0 if the exporter origin is the same as the importer destionation\n",
    "data.loc[data['iso_d'] == data['iso_o'], 'dist'] = 0\n",
    "\n",
    "\n",
    "# Find indices where 'trade_value' is not NaN and 'total_export_value' is greater than 10^7\n",
    "z_vals = data[(~data['trade_value'].isna()) & (data['total_export_value'] > 10**7)].index\n",
    "\n",
    "\n",
    "# Apply log transformation\n",
    "data_log = data.copy()\n",
    "log_transform_columns = ['trade_value', 'total_export_value', 'total_category_value',\n",
    "                         'iso_o_gdp', 'iso_d_gdp', 'dist',\n",
    "                         'iso_o_internet', 'iso_d_internet', 'iso_o_fixed_broadband', \n",
    "                         'iso_d_fixed_broadband', 'iso_o_mobile_broadband', 'iso_d_mobile_broadband',\n",
    "                         'revenues_iso_o']\n",
    "\n",
    "for col in log_transform_columns:\n",
    "    data_log[col] = np.log(1 + data_log[col])\n",
    "\n",
    "\n",
    "# Apply categorical transformation\n",
    "categorical_cols = ['year', 'category', 'iso_o_loc', 'iso_d_loc']\n",
    "for col in categorical_cols:\n",
    "    data_log[col] = data_log[col].astype('category')  \n",
    "\n",
    "# Get the data used for regression\n",
    "data_regression = data_log.loc[z_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5545a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REGRESSION TREE CROSS-VALIDATION\n",
    "\n",
    "\n",
    "# One hot encoding of categorical variables\n",
    "X = data_regression.copy()\n",
    "\n",
    "for col in ['iso_o_loc', 'iso_d_loc', 'year']:  # list all your categorical columns\n",
    "    dummies = pd.get_dummies(data_regression[col], prefix=col, drop_first=False)\n",
    "    X = pd.concat([X, dummies], axis=1).drop(col, axis=1)\n",
    "\n",
    "y = data_regression['trade_value']  \n",
    "X.drop(['category', 'firm', 'iso_o', 'iso_d', 'trade_value'], axis=1, inplace=True)\n",
    "\n",
    "# Set baseline categories for logistic regression\n",
    "X_log_reg = X.copy()\n",
    "X_log_reg.drop(['iso_o_loc_North America', 'iso_d_loc_North America', 'year_2016'], axis=1, inplace=True)\n",
    "\n",
    "# Create the group variable\n",
    "groups = data_regression['firm'].astype(str) + '_' + data_regression['iso_o'].astype(str) + '_' + data_regression['category'].astype(str)\n",
    "groups = groups.astype(str).factorize()[0]\n",
    "\n",
    "# Initialize GroupKFold\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Parameter grid for GridSearchCV\n",
    "param_grid = {'max_depth': [1, 3, 5, 7], \n",
    "              'min_child_weight': [5, 20, 50, 100, 200]}\n",
    "\n",
    "# For storing the results\n",
    "mse_scores_per_set = {tuple(sorted(param.items())): [] for param in ParameterGrid(param_grid)}\n",
    "mse_train_scores_per_set = {tuple(sorted(param.items())): [] for param in ParameterGrid(param_grid)}\n",
    "\n",
    "# Iterate over each fold\n",
    "fold_num = 0\n",
    "for train_index, test_index in gkf.split(X, y, groups=groups):\n",
    "    fold_num += 1\n",
    "    X_train, X_test = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
    "    X_log_reg_train, X_log_reg_test = X_log_reg.iloc[train_index].copy(), X_log_reg.iloc[test_index].copy()\n",
    "    \n",
    "    y_train, y_test = y.iloc[train_index].copy(), y.iloc[test_index].copy()\n",
    "\n",
    "    # Fit logistic regression for zeroProbabilities\n",
    "    log_reg = LogisticRegression(max_iter=2000)\n",
    "    log_reg.fit(X_log_reg_train, (y_train > 0))\n",
    "    X_train['zeroProbabilities'] = log_reg.predict_proba(X_log_reg_train)[:, 1]\n",
    "    X_test['zeroProbabilities'] = log_reg.predict_proba(X_log_reg_test)[:, 1]\n",
    "\n",
    "    # Manually iterate over each combination of hyperparameters\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        \n",
    "        estimator = XGBRegressor(**params, n_estimators=150, learning_rate=0.10,\n",
    "                                  random_state=42)\n",
    "        estimator.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = estimator.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        y_fit = estimator.predict(X_train)\n",
    "        mse_train = mean_squared_error(y_train, y_fit)\n",
    "        \n",
    "        # Print fold number, parameters, and MSE\n",
    "        print(f\"Fold {fold_num}, Params: {params}, MSE: {mse}, MSEtrain: {mse_train}\")\n",
    "        \n",
    "        mse_scores_per_set[tuple(sorted(params.items()))].append(mse)\n",
    "        mse_train_scores_per_set[tuple(sorted(params.items()))].append(mse_train)\n",
    "\n",
    "\n",
    "# Calculate average MSE for each parameter set\n",
    "avg_mse_scores = {params: np.mean(mse_scores) for params, mse_scores in mse_scores_per_set.items()}\n",
    "avg_mse_train_scores = {params: np.mean(mse_train_scores) for params, mse_train_scores in mse_train_scores_per_set.items()}\n",
    "\n",
    "# Determine the best parameter set\n",
    "best_params = min(avg_mse_scores, key=avg_mse_scores.get)\n",
    "best_score = avg_mse_scores[best_params]\n",
    "best_train_score = avg_mse_train_scores[best_params]\n",
    "\n",
    "print(\"Best parameters based on average MSE across folds:\", dict(best_params))\n",
    "print(\"Best average MSE:\", best_score)\n",
    "print(\"Best train average MSE:\", best_train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1279bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT CROSS-VALIDATION HEATMAP\n",
    "\n",
    "# Assume avg_mse_scores is available in your environment\n",
    "\n",
    "# Convert to DataFrame and pivot\n",
    "avg_mse_df = pd.DataFrame.from_dict(avg_mse_scores, orient='index', columns=['avg_mse'])\n",
    "avg_mse_df['max_depth'] = avg_mse_df.index.map(lambda x: dict(x)['max_depth'])\n",
    "avg_mse_df['min_child_weight'] = avg_mse_df.index.map(lambda x: dict(x)['min_child_weight'])\n",
    "avg_mse_pivot = avg_mse_df.pivot('max_depth', 'min_child_weight', 'avg_mse')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(avg_mse_pivot, annot=True, fmt=\".3f\", cmap=\"viridis\")\n",
    "plt.title(\"Average MSE for Each Hyperparameter Combination\")\n",
    "plt.xlabel(\"Min Child Weight\")\n",
    "plt.ylabel(\"Max Depth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6faca89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## COMPARE WITH LINEAR REGRESSION\n",
    "\n",
    "# One hot encoding of categorical variables\n",
    "X = data_regression.copy()\n",
    "\n",
    "for col in ['iso_o_loc', 'iso_d_loc', 'year']:  # list all your categorical columns\n",
    "    dummies = pd.get_dummies(data_regression[col], prefix=col, drop_first=False)\n",
    "    X = pd.concat([X, dummies], axis=1).drop(col, axis=1)\n",
    "    \n",
    "y = data_regression['trade_value']  \n",
    "X.drop(['category', 'firm', 'iso_o', 'iso_d', 'trade_value'], axis=1, inplace=True)\n",
    "\n",
    "# Remove baseline categories\n",
    "X.drop(['iso_o_loc_North America', 'iso_d_loc_North America', 'year_2016'], axis=1, inplace=True)\n",
    "\n",
    "# For storing MSE values\n",
    "mse_scores_lin_reg = []\n",
    "\n",
    "# Iterate over each fold\n",
    "fold_num = 0\n",
    "for train_index, test_index in gkf.split(X, y, groups=groups):\n",
    "    fold_num += 1\n",
    "    X_train, X_test = X.iloc[train_index].copy(), X.iloc[test_index].copy()\n",
    "    y_train, y_test = y.iloc[train_index].copy(), y.iloc[test_index].copy()\n",
    "\n",
    "    # Fit the linear regression model\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set and calculate MSE\n",
    "    y_pred = lin_reg.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # Store and print MSE\n",
    "    mse_scores_lin_reg.append(mse)\n",
    "\n",
    "    print(f\"Fold {fold_num}, MSE: {mse}\")\n",
    "\n",
    "# Calculate the average MSE across all folds\n",
    "avg_mse_lin_reg = np.mean(mse_scores_lin_reg)\n",
    "print(f\"Average MSE for Linear Regression: {avg_mse_lin_reg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET FINAL PREDICTIONS\n",
    "\n",
    "\n",
    "X = data_regression.copy()\n",
    "\n",
    "for col in ['iso_o_loc', 'iso_d_loc', 'year']:  # list all your categorical columns\n",
    "    dummies = pd.get_dummies(data_regression[col], prefix=col, drop_first=False)\n",
    "    X = pd.concat([X, dummies], axis=1).drop(col, axis=1)\n",
    "\n",
    "y = data_regression['trade_value']  \n",
    "X.drop(['category', 'firm', 'iso_o', 'iso_d', 'trade_value'], axis=1, inplace=True)\n",
    "\n",
    "X_log_reg = X.copy()\n",
    "X_log_reg.drop(['iso_o_loc_North America', 'iso_d_loc_North America', 'year_2016'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Fit logistic regression for zeroProbabilities on all data\n",
    "log_reg = LogisticRegression(max_iter=2000)\n",
    "log_reg.fit(X_log_reg, (y > 0))\n",
    "zero_prob = log_reg.predict_proba(X_log_reg)[:, 1]\n",
    "\n",
    "# Add zeroProbabilities to the feature set\n",
    "X['zeroProbabilities'] = zero_prob\n",
    "\n",
    "X_prediction = data_log.copy()\n",
    "\n",
    "for col in ['iso_o_loc', 'iso_d_loc', 'year']:  # list all your categorical columns\n",
    "    dummies = pd.get_dummies(data_log[col], prefix=col, drop_first=False)\n",
    "    X_prediction = pd.concat([X_prediction, dummies], axis=1).drop(col, axis=1)\n",
    "\n",
    "X_prediction.drop(['category', 'firm', 'iso_o', 'iso_d', 'trade_value'], axis=1, inplace=True)\n",
    "\n",
    "X_prediction_log_reg = X_prediction.copy()\n",
    "X_prediction_log_reg.drop(['iso_o_loc_North America', 'iso_d_loc_North America', 'year_2016'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Fit logistic regression for zeroProbabilities on all data\n",
    "zero_prob = log_reg.predict_proba(X_prediction_log_reg)[:, 1]\n",
    "\n",
    "# Add zeroProbabilities to the feature set\n",
    "X_prediction['zeroProbabilities'] = zero_prob\n",
    "\n",
    "# Fit the GBRT\n",
    "gbr = XGBRegressor(learning_rate=0.1, n_estimators=150,\n",
    "                       max_depth=best_params[0][1],\n",
    "                       min_child_weight=best_params[1][1],\n",
    "                       random_state=42)\n",
    "gbr.fit(X, y)\n",
    "\n",
    "\n",
    "# Fit the linear regression model\n",
    "lin_reg.fit(X_log_reg, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data['ypred_combined'] = gbr.predict(X_prediction)\n",
    "data['ypred_linear_combined'] = lin_reg.predict(X_prediction_log_reg)\n",
    "\n",
    "\n",
    "columns_to_keep = ['firm', 'year', 'category', 'iso_o', 'iso_d', 'trade_value', 'total_export_value', 'ypred_combined', 'ypred_linear_combined']\n",
    "\n",
    "# Select only these columns\n",
    "TT_reduced = data[columns_to_keep].copy()\n",
    "\n",
    "# Create non-normalized predictions\n",
    "TT_reduced['trade_value_pred'] = np.exp(TT_reduced['ypred_combined']) - 1\n",
    "TT_reduced.loc[TT_reduced['trade_value_pred'] < 1000, 'trade_value_pred'] = 0\n",
    "\n",
    "TT_reduced['trade_value_linear_pred'] = np.exp(TT_reduced['ypred_linear_combined']) - 1\n",
    "TT_reduced.loc[TT_reduced['trade_value_linear_pred'] < 1000, 'trade_value_linear_pred'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a8cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## POST PROCESSING # 1 - summing up to firm revenues\n",
    "\n",
    "# Calculate normalized values for categories other than 'apps' and 'games'\n",
    "non_app_game_mask = ~TT_reduced['category'].isin(['apps', 'games'])\n",
    "grouped = TT_reduced[non_app_game_mask].groupby(['firm', 'year', 'category', 'iso_o'])\n",
    "TT_reduced.loc[non_app_game_mask, 'normalized_trade_value_pred'] = grouped['trade_value_pred'].transform(lambda x: x / x.sum() * TT_reduced.loc[x.index, 'total_export_value'])\n",
    "TT_reduced.loc[non_app_game_mask, 'normalized_trade_value_linear_pred'] = grouped['trade_value_linear_pred'].transform(lambda x: x / x.sum() * TT_reduced.loc[x.index, 'total_export_value'])\n",
    "\n",
    "# For 'apps' and 'games' categories, set normalized_trade_value_pred based on the condition\n",
    "app_game_mask = TT_reduced['category'].isin(['apps', 'games'])\n",
    "TT_reduced.loc[app_game_mask, 'normalized_trade_value_pred'] = TT_reduced.loc[app_game_mask, 'trade_value'].fillna(TT_reduced.loc[app_game_mask, 'trade_value_pred'])\n",
    "TT_reduced.loc[app_game_mask, 'normalized_trade_value_linear_pred'] = TT_reduced.loc[app_game_mask, 'trade_value'].fillna(TT_reduced.loc[app_game_mask, 'trade_value_linear_pred'])\n",
    "\n",
    "## POST-PROCESSING #2 - sum up to regional revenues\n",
    "\n",
    "# load firm revenue shares\n",
    "firm_revenue_shares = pd.read_excel('firm-revenue-share.xlsx', sheet_name='shares')\n",
    "firm_revenue_regions = pd.read_excel('firm-revenue-share.xlsx', sheet_name='regions')\n",
    "\n",
    "# Calculate the total revenue for each firm-category-year group\n",
    "total_revenue = TT_reduced.groupby(['firm','iso_o', 'category', 'year'])['normalized_trade_value_pred'].sum().reset_index(name='total_revenue')\n",
    "total_not_revenue = TT_reduced.groupby(['firm','iso_o', 'category', 'year'])['trade_value_pred'].sum().reset_index(name='total_not_revenue')\n",
    "\n",
    "\n",
    "# Step 1. calculate revenue share for apps and games\n",
    "\n",
    "# Step 1.1: Merge this total revenue back into TT_reduced\n",
    "TT_reduced = TT_reduced.merge(total_revenue, on=['firm', 'iso_o', 'category', 'year'])\n",
    "TT_reduced = TT_reduced.merge(total_not_revenue, on=['firm', 'iso_o', 'category', 'year'])\n",
    "app_game_mask = TT_reduced['category'].isin(['apps', 'games'])\n",
    "\n",
    "# Step 1.2: Use the mask with .loc to update 'total_not_revenue' where the condition is True\n",
    "TT_reduced.loc[app_game_mask, 'total_not_revenue'] = TT_reduced.loc[app_game_mask, 'total_revenue'].astype('float32')\n",
    "\n",
    "# Step 1.3: Calculate the revenue_share\n",
    "TT_reduced['revenue_share'] = TT_reduced['trade_value_pred'] / TT_reduced['total_not_revenue']\n",
    "TT_reduced.loc[app_game_mask, 'revenue_share'] = (TT_reduced.loc[app_game_mask, 'normalized_trade_value_pred'] / TT_reduced.loc[app_game_mask, 'total_revenue']).astype('float32')\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Adjust for countries with known consumption\n",
    "\n",
    "# Step 2.1: Merge TT_reduced with firm_revenue_shares\n",
    "TT_reduced = pd.merge(TT_reduced, firm_revenue_shares, how='left', left_on=['firm', 'iso_d'], right_on=['firm', 'region_iso_d'], suffixes=('', '_frs'))\n",
    "\n",
    "# Step 2.2: Compute the final_revenue_share\n",
    "TT_reduced['final_revenue_share'] = TT_reduced['revenue_share_frs'].where(pd.notnull(TT_reduced['revenue_share_frs']), None)\n",
    "\n",
    "# Drop the extra columns introduced by the merge\n",
    "TT_reduced.drop(['region_iso_d', 'revenue_share_frs'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Step 3. Adjust for regions with known consumption\n",
    "\n",
    "# Step 3.1: Pre-compute the mapping from regions to countries\n",
    "region_to_countries = {region: firm_revenue_regions[region].dropna().unique().tolist()\n",
    "                       for region in firm_revenue_regions.columns}\n",
    "\n",
    "# Step 3.2: Identify rows in firm_revenue_shares not included in the first step\n",
    "unprocessed_frs = firm_revenue_shares[~firm_revenue_shares['region_iso_d'].isin(TT_reduced['iso_d'].unique())]\n",
    "\n",
    "\n",
    "# Calculate the total number of iterations for progress tracking\n",
    "total_iterations = len(unprocessed_frs)\n",
    "\n",
    "# Step 3.3: Iterate over unprocessed firm_revenue_shares and renormalize revenue shares\n",
    "for i, (index, frs_row) in enumerate(unprocessed_frs.iterrows(), 1):\n",
    "    firm = frs_row['firm']\n",
    "    region_iso_d = frs_row['region_iso_d']\n",
    "    target_revenue_share = frs_row['revenue_share']\n",
    "\n",
    "    if region_iso_d in region_to_countries:\n",
    "        region_vals = region_to_countries[region_iso_d]\n",
    "\n",
    "        # Filter TT_reduced for the current firm and get unique categories\n",
    "        firm_categories = TT_reduced[TT_reduced['firm'] == firm]['category'].unique()\n",
    "\n",
    "        for year in TT_reduced['year'].unique():\n",
    "            for category in firm_categories:\n",
    "                mask = (TT_reduced['firm'] == firm) & (TT_reduced['year'] == year) & \\\n",
    "                       (TT_reduced['category'] == category) & (TT_reduced['iso_d'].isin(region_vals))\n",
    "\n",
    "                relevant_rows = TT_reduced[mask]\n",
    "                total_share = relevant_rows['revenue_share'].sum()\n",
    "\n",
    "                if total_share > 0:\n",
    "                    scaling_factor = target_revenue_share / total_share\n",
    "                    TT_reduced.loc[mask, 'final_revenue_share'] = relevant_rows['revenue_share'] * scaling_factor\n",
    "\n",
    "    # Display the percentage completed\n",
    "    percentage_completed = (i / total_iterations) * 100\n",
    "    print(f\"Completed: {percentage_completed:.2f}%\")\n",
    "\n",
    "# Step 4: Normalize the values for each region/country and product with unknown regional data\n",
    "\n",
    "# Step 4.1: Identify firms in TT_reduced not present in firm_revenue_shares\n",
    "unique_firms_in_frs = firm_revenue_shares['firm'].unique()\n",
    "firms_not_in_frs = TT_reduced.loc[~TT_reduced['firm'].isin(unique_firms_in_frs), 'firm'].unique()\n",
    "\n",
    "# Step 4.2: \n",
    "# Set final_revenue_share equal to revenue_share for these firms\n",
    "TT_reduced.loc[TT_reduced['firm'].isin(firms_not_in_frs), 'final_revenue_share'] = TT_reduced['revenue_share']\n",
    "\n",
    "# Group by 'firm', 'category', 'year' and calculate the sum of 'final_revenue_share' \n",
    "# Step 4.3: Identify groups with at least one NaN in final_revenue_share\n",
    "groups_with_nan = TT_reduced[TT_reduced['final_revenue_share'].isna()].groupby(['firm', 'iso_o', 'category', 'year']).size().reset_index(name='count').drop(columns='count')\n",
    "\n",
    "# Step 4.4: Calculate the sum of final_revenue_share for each group, excluding NaN values\n",
    "grouped_sum = TT_reduced.groupby(['firm', 'iso_o', 'category', 'year'])['final_revenue_share'].sum(min_count=1).reset_index()\n",
    "\n",
    "# Step 4.5: Merge to include only groups identified in step 4.4\n",
    "grouped_sum = groups_with_nan.merge(grouped_sum, on=['firm', 'iso_o', 'category', 'year'], how='left')\n",
    "\n",
    "# Merge this back to TT_reduced only for rows where 'final_revenue_share' is NaN\n",
    "TT_reduced = TT_reduced.merge(grouped_sum, on=['firm', 'iso_o', 'category', 'year'], how='left', suffixes=('', '_group_sum'))\n",
    "\n",
    "# Calculate the total number of iterations for progress tracking\n",
    "total_iterations = len(grouped_sum)\n",
    "\n",
    "# Iterate over each group and normalize NaN entries in 'final_revenue_share'\n",
    "for i, row in enumerate(grouped_sum.iterrows(), 1):\n",
    "    firm = row[1]['firm']\n",
    "    category = row[1]['category']\n",
    "    year = row[1]['year']\n",
    "    group_sum = row[1]['final_revenue_share']\n",
    "\n",
    "    mask = (TT_reduced['firm'] == firm) & (TT_reduced['category'] == category) & (TT_reduced['year'] == year) & TT_reduced['final_revenue_share'].isna()\n",
    "    \n",
    "    # Calculate the sum of 'revenue_share' for NaN entries\n",
    "    nan_sum = TT_reduced.loc[mask, 'revenue_share'].sum()\n",
    "\n",
    "    # Normalize NaN entries\n",
    "    if nan_sum > 0:\n",
    "        normalization_factor = (1 - group_sum) / nan_sum\n",
    "        TT_reduced.loc[mask, 'final_revenue_share'] = TT_reduced.loc[mask, 'revenue_share'] * normalization_factor\n",
    "\n",
    "    # Display the percentage completed\n",
    "    percentage_completed = (i / total_iterations) * 100\n",
    "    print(f\"Completed: {percentage_completed:.2f}%\")\n",
    "\n",
    "# Drop the extra '_group_sum' column\n",
    "TT_reduced.drop('final_revenue_share_group_sum', axis=1, inplace=True)\n",
    "\n",
    "TT_reduced['final_revenue_share'] = TT_reduced['final_revenue_share'].apply(lambda x: 0 if x < 10**-8 else x)\n",
    "\n",
    "estimated_rev_share = TT_reduced.groupby(['firm','iso_o', 'category', 'year'])['final_revenue_share'].sum().reset_index(name='estimated_rev_share')\n",
    "TT_reduced = TT_reduced.merge(estimated_rev_share, on=['firm', 'iso_o', 'category', 'year'])\n",
    "TT_reduced['final_revenue_share'] = TT_reduced['final_revenue_share'] / TT_reduced['estimated_rev_share']\n",
    "TT_reduced['normalized_trade_value_pred'] = TT_reduced['final_revenue_share'] * TT_reduced['total_revenue']\n",
    "TT_reduced['year'] = TT_reduced['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be198075",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT REVENUE SHARE\n",
    "\n",
    "# Assuming TT_reduced_2, firm_revenue_shares, and firm_revenue_regions are already loaded DataFrames\n",
    "TT_reduced_2 = TT_reduced.copy()\n",
    "firm_revenue_shares = pd.read_excel('firm-revenue-share.xlsx', sheet_name='shares')\n",
    "\n",
    "# Filter TT_reduced_2 to include only firms present in firm_revenue_shares\n",
    "relevant_firms = firm_revenue_shares['firm'].unique()\n",
    "TT_reduced_2_filtered = TT_reduced_2[TT_reduced_2['firm'].isin(relevant_firms)]\n",
    "\n",
    "# Filter TT_reduced_2 to include only data for the year 2021\n",
    "TT_reduced_2021 = TT_reduced_2_filtered[TT_reduced_2_filtered['year'] == 2021]\n",
    "\n",
    "# Group by 'firm' and sum up 'normalized_trade_value_pred' and 'normalized_trade_value_pred_linear' across all categories\n",
    "firm_summaries = TT_reduced_2021.groupby(['firm','iso_d'])[['trade_value_pred', 'trade_value_linear_pred','normalized_trade_value_pred']].sum().reset_index()\n",
    "\n",
    "# Step 1: Calculate Total Revenue per Firm\n",
    "total_revenue = firm_summaries.groupby('firm')[['trade_value_pred', 'trade_value_linear_pred', 'normalized_trade_value_pred']].sum()\n",
    "\n",
    "# Step 2: Calculate Revenue Share for Each Region\n",
    "def calculate_revenue_share(row, summaries, total_revenue, column_name):\n",
    "    firm = row['firm']\n",
    "    region_iso_d = row['region_iso_d']\n",
    "    if region_iso_d in summaries['iso_d'].unique():\n",
    "        region_sum = summaries[(summaries['firm'] == firm) & (summaries['iso_d'] == region_iso_d)][column_name].sum()\n",
    "    else:\n",
    "        # Use the firm_revenue_regions DataFrame\n",
    "        alternative_regions = firm_revenue_regions[region_iso_d].unique()\n",
    "        region_sum = summaries[(summaries['firm'] == firm) & (summaries['iso_d'].isin(alternative_regions))][column_name].sum()\n",
    "    total_firm_revenue = total_revenue.loc[firm, column_name]\n",
    "    return region_sum / total_firm_revenue if total_firm_revenue != 0 else 0\n",
    "\n",
    "firm_revenue_shares['revenue_share_predicted'] = firm_revenue_shares.apply(lambda row: calculate_revenue_share(row, firm_summaries, total_revenue, 'trade_value_pred'), axis=1)\n",
    "firm_revenue_shares['revenue_share_linear'] = firm_revenue_shares.apply(lambda row: calculate_revenue_share(row, firm_summaries, total_revenue, 'trade_value_linear_pred'), axis=1)\n",
    "firm_revenue_shares['revenue_share_final'] = firm_revenue_shares.apply(lambda row: calculate_revenue_share(row, firm_summaries, total_revenue, 'normalized_trade_value_pred'), axis=1)\n",
    "\n",
    "# Step 3: The firm_revenue_shares DataFrame now has the new columns 'revenue_share' and 'revenue_share_linear' added\n",
    "\n",
    "# Creating the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='revenue_share', y='revenue_share_predicted', data=firm_revenue_shares)\n",
    "\n",
    "# Calculating the correlation\n",
    "correlation = firm_revenue_shares['revenue_share'].corr(firm_revenue_shares['revenue_share_predicted'])\n",
    "# Calculating the R-squared value\n",
    "\n",
    "# Adding the title with the correlation value\n",
    "plt.title(f'Scatter Plot with Correlation: {correlation:.2f}')\n",
    "plt.xlabel('Revenue Share')\n",
    "plt.ylabel('Revenue Share Predicted')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "firm_revenue_shares.to_csv('predicted_shares.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb04303",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIMAL TRANSPORT\n",
    "\n",
    "# Load distance data\n",
    "data_distance = pd.read_excel('dist_data.xlsx')\n",
    "data_distance = data_distance[['iso_o', 'iso_d', 'dist']]\n",
    "\n",
    "# Set 'dist' to 1 where 'iso_o' is equal to 'iso_d'\n",
    "data_distance.loc[data_distance['iso_o'] == data_distance['iso_d'], 'dist'] = 0\n",
    "\n",
    "# Load firm revenues by subsidiary_data\n",
    "firm_subsidiary_revenues = pd.read_csv('firm_revenues_by_subsidiary.csv')\n",
    "    \n",
    "def dpt_optimal_transport(Q, R, D):\n",
    "    # Normalize revenues and consumption\n",
    "    R = 1e4 * R / np.sum(R)\n",
    "    Q = 1e4 * Q / np.sum(Q)\n",
    "\n",
    "    # Reshape and normalize Phi_vec\n",
    "    Phi_vec = D.flatten()\n",
    "    Phi_vec = Phi_vec\n",
    "\n",
    "    # Create sparse matrices for constraints\n",
    "    num_R = len(R)\n",
    "    num_Q = len(Q)\n",
    "    Aeq_top = kron(csr_matrix(np.ones((1, num_R))), eye(num_Q))\n",
    "    Aeq_bottom = kron(eye(num_R), csr_matrix(np.ones((1, num_Q))))\n",
    "    Aeq = vstack([Aeq_top, Aeq_bottom], format='csr')\n",
    "    beq = np.concatenate([Q, R])\n",
    "\n",
    "    # Define bounds, and linear programming\n",
    "    W0 = np.ones_like(Phi_vec) / len(Phi_vec)\n",
    "    lb = np.zeros_like(W0)\n",
    "    ub = 1e4 * np.ones_like(W0)\n",
    "    res = linprog(-Phi_vec, A_eq=Aeq, b_eq=beq, bounds=list(zip(lb, ub)), method='highs')\n",
    "    \n",
    "    if not res.success:\n",
    "        raise ValueError(f\"Linear programming did not succeed: {res.message}\")\n",
    "\n",
    "\n",
    "    # Reshape the result\n",
    "    W_opt = res.x.reshape((num_R, num_Q))\n",
    "\n",
    "    return W_opt\n",
    "\n",
    "def estimate_optimal_transport(year, TT, data_distance):\n",
    "    TT_reduced = TT[TT['year'] == year].copy()\n",
    "\n",
    "    TT_results_v2 = firm_subsidiary_revenues[firm_subsidiary_revenues['year'] == year].copy()\n",
    "\n",
    "    # Selecting the columns by their Python indices\n",
    "    selected_columns = TT_results_v2.iloc[:, [2, 3, 5]]\n",
    "\n",
    "    # Getting unique rows (combinations) from these columns\n",
    "    parents_categories = selected_columns.drop_duplicates()\n",
    "    # Preprocess data_distance for quick lookup\n",
    "\n",
    "    # Create a dictionary to map (iso_o, iso_d) to dist\n",
    "    dist_dict = {(row['iso_o'], row['iso_d']): row['dist'] for _, row in data_distance.iterrows()}\n",
    "\n",
    "    all_TT_optimal = []  # List to store results for each i\n",
    "    total_length = len(parents_categories)  # Total number of iterations\n",
    "    optimized_firms_categories = []  # List to store firms and categories for which optimization was done\n",
    "\n",
    "    \n",
    "    for i in range(len(parents_categories)):\n",
    "        print(f\"Processing {i+1}/{total_length} ({(i+1)/total_length*100:.2f}%)\")\n",
    "\n",
    "        parent = parents_categories.iloc[i]['parent']\n",
    "        category = parents_categories.iloc[i]['category']\n",
    "    \n",
    "        z_TT = TT_reduced[(TT_reduced['firm'] == parent) & (TT_reduced['category'] == category)]\n",
    "        z_TT_results = TT_results_v2[(TT_results_v2['parent'] == parent) & (TT_results_v2['category'] == category)]\n",
    "\n",
    "        # Group by 'iso_o' and sum 'value' for each iso_o, then remove entries with sum 0\n",
    "        rev_TT_results = z_TT_results.groupby('iso_o')['value'].sum()\n",
    "        rev_TT_results = rev_TT_results[rev_TT_results != 0]\n",
    "        iso_o = rev_TT_results.index\n",
    "\n",
    "        # Group by 'iso_d' and sum 'normalized_trade_value_pred' for each iso_d, then remove entries with sum 0\n",
    "        rev_TT = z_TT.groupby('iso_d')['normalized_trade_value_pred'].sum()\n",
    "        rev_TT = rev_TT[rev_TT != 0]\n",
    "        iso_d = rev_TT.index\n",
    "        \n",
    "\n",
    "        if len(iso_o) > 1 and rev_TT.sum() >0:\n",
    "            \n",
    "            # Perform operations if there are more than one unique iso_o values\n",
    "            optimized_firms_categories.append((parent, category))\n",
    "\n",
    "\n",
    "            # Initialize and fill the cost matrix\n",
    "            cost_matrix = np.full((len(iso_o), len(iso_d)), np.nan)\n",
    "            for idx_o, o in enumerate(iso_o):\n",
    "                for idx_d, d in enumerate(iso_d):\n",
    "                    dist = dist_dict.get((o, d))\n",
    "                    if dist is not None:\n",
    "                        cost_matrix[idx_o, idx_d] = 1 / (1 + dist)\n",
    "                               \n",
    "            cost_matrix *= 10**4\n",
    "\n",
    "            # Optimal transport calculation\n",
    "            W_max = max(sum(rev_TT), sum(rev_TT_results))\n",
    "            W_opt = W_max * dpt_optimal_transport(rev_TT, rev_TT_results, cost_matrix) / 10**4 \n",
    "            \n",
    "\n",
    "            # Data preparation for DataFrame\n",
    "            firm_vals = np.repeat(parent, len(iso_o) * len(iso_d))\n",
    "            year_vals = np.repeat(parents_categories.iloc[i]['year'], len(iso_o) * len(iso_d))\n",
    "            category_vals = np.repeat(category, len(iso_o) * len(iso_d))\n",
    "            normalized_trade_value_pred = W_opt.flatten()\n",
    "            iso_o_v2 = np.tile(iso_o, (len(iso_d), 1)).T.flatten()\n",
    "            iso_d_v2 = np.tile(iso_d, (len(iso_o), 1)).flatten()\n",
    "\n",
    "            # Creating and storing the DataFrame\n",
    "            TT_optimal = pd.DataFrame({\n",
    "                'firm': firm_vals,\n",
    "                'year': year_vals,\n",
    "                'category': category_vals,\n",
    "                'iso_o': iso_o_v2,\n",
    "                'iso_d': iso_d_v2,\n",
    "                'normalized_trade_value_pred': normalized_trade_value_pred\n",
    "            })\n",
    "            all_TT_optimal.append(TT_optimal)\n",
    "\n",
    "    # Concatenate all DataFrames in the list to a single DataFrame\n",
    "    all_TT_optimal_df = pd.concat(all_TT_optimal, ignore_index=True)\n",
    "\n",
    "    columns_to_keep = ['firm', 'year', 'category', 'iso_o', 'iso_d', 'normalized_trade_value_pred']\n",
    "\n",
    "    # Select only these columns\n",
    "    TT_reduced = TT_reduced[columns_to_keep]\n",
    "\n",
    "\n",
    "    # Convert the list to a DataFrame for easier processing\n",
    "    optimized_df = pd.DataFrame(optimized_firms_categories, columns=['parent', 'category'])\n",
    "\n",
    "    # Create a condition to identify rows to be removed from TT_reduced\n",
    "    condition_df = TT_reduced[['firm', 'category']].merge(optimized_df, left_on=['firm', 'category'], right_on=['parent', 'category'], how='left', indicator=True)\n",
    "    condition_to_remove = condition_df['_merge'] == 'both'\n",
    "\n",
    "    # Reset the index of TT_reduced to align with the condition\n",
    "    TT_reduced.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Filter out these rows from TT_reduced\n",
    "    TT_reduced_filtered = TT_reduced[~condition_to_remove.values]\n",
    "\n",
    "    # Concatenate TT_reduced_filtered with all_TT_optimal_df\n",
    "    final_result_df = pd.concat([TT_reduced_filtered, all_TT_optimal_df], ignore_index=True)\n",
    "\n",
    "    # At the end of the code, return final_result_df for the year\n",
    "    return final_result_df\n",
    "\n",
    "final_results_ot = {}\n",
    "for year in range(2016, 2022):\n",
    "    final_results_ot[year] = estimate_optimal_transport(year,TT_reduced,data_distance)\n",
    "    \n",
    "final_results_hq = {}\n",
    "for year in range(2016, 2022):\n",
    "    # Filter TT_reduced for the current year and select specific columns\n",
    "    final_results_hq[year] = TT_reduced[\n",
    "        (TT_reduced['year'] == year)\n",
    "    ][['firm', 'year', 'category', 'iso_o', 'iso_d', 'normalized_trade_value_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d60b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CALCULATE CONFIDENCE INTERVALS & EXPORT FINAL RESULTS\n",
    "\n",
    "def calculate_confidence_interval(TT):\n",
    "    TT = TT[TT['normalized_trade_value_pred'] > 0]\n",
    "\n",
    "    # Grouping by 'firm' and 'iso_o' and calculating the sum\n",
    "    grouped_sum = TT.groupby(['firm', 'iso_o', 'category'])['normalized_trade_value_pred'].sum().reset_index()\n",
    "    grouped_sum = grouped_sum.rename(columns={'normalized_trade_value_pred': 'total_revenues'})\n",
    "\n",
    "    # Merging the sum back into the original DataFrame\n",
    "    TT = TT.merge(grouped_sum, on=['firm', 'iso_o', 'category'], how='left')\n",
    "\n",
    "    # Transforming the target variable\n",
    "    TT['log_normalized_trade_value_pred'] = np.log(1 + TT['normalized_trade_value_pred'])\n",
    "    TT['log_total_revenues'] = np.log(1 + TT['total_revenues'])\n",
    "\n",
    "    # Convert 'iso_o' to a categorical type with the baseline level first\n",
    "    TT['iso_o'] = pd.Categorical(TT['iso_o'], categories=['USA'] + [x for x in TT['iso_o'].unique() if x != 'USA'], ordered=True)\n",
    "\n",
    "\n",
    "    # Adjusted approach to fit the initial model using the reordered 'iso_o'\n",
    "    initial_formula = 'log_normalized_trade_value_pred ~ log_total_revenues + C(iso_o, Treatment(reference=\"USA\")) + C(iso_d, Treatment(reference=\"USA\"))'\n",
    "    initial_model = smf.ols(initial_formula, data=TT).fit()\n",
    "\n",
    "    # Identify significant levels in iso_o (p-value < 0.05)\n",
    "    significant_iso_o_levels = ['USA']  # Dynamically selected baseline included\n",
    "\n",
    "    for level in TT['iso_o'].cat.categories:  # Corrected access to categories\n",
    "        if level == 'USA':\n",
    "            continue\n",
    "        else:\n",
    "            coefficient_name = f'C(iso_o, Treatment(reference=\"USA\"))[T.{level}]'\n",
    "            if coefficient_name in initial_model.pvalues and initial_model.pvalues[coefficient_name] < 0.05:\n",
    "                significant_iso_o_levels.append(level)\n",
    "\n",
    "    # Recoding insignificant levels to 'Other'\n",
    "    TT['iso_o_reduced'] = TT['iso_o'].apply(lambda x: x if x in significant_iso_o_levels else 'Other').astype('category')\n",
    "\n",
    "    # Fit the model using the modified iso_o column\n",
    "    formula = 'log_normalized_trade_value_pred ~ log_total_revenues + C(iso_o_reduced, Treatment(reference=\"USA\")) + C(iso_d, Treatment(reference=\"USA\"))'\n",
    "    model = smf.ols(formula, data=TT).fit()\n",
    "\n",
    "    # Predicting values\n",
    "    TT['predicted'] = model.predict()\n",
    "\n",
    "    # Calculating confidence intervals\n",
    "    predictions = model.get_prediction(TT)\n",
    "    ci = predictions.summary_frame(alpha=0.05)[['mean_ci_lower', 'mean_ci_upper']]\n",
    "\n",
    "    # Adding confidence intervals back to the DataFrame\n",
    "    ci.reset_index(drop=True, inplace=True)\n",
    "    TT.reset_index(drop=True, inplace=True)\n",
    "    TT = pd.concat([TT, ci], axis=1)\n",
    "\n",
    "    # Adjustments for predicted values and confidence intervals\n",
    "    TT['predicted'] = np.exp(TT['predicted']) - 1\n",
    "    TT['mean_ci_lower'] = (np.exp(TT['mean_ci_lower']) - 1) * TT['normalized_trade_value_pred'] / TT['predicted']\n",
    "    TT['mean_ci_upper'] = (np.exp(TT['mean_ci_upper']) - 1) * TT['normalized_trade_value_pred'] / TT['predicted']\n",
    "\n",
    "    # Selecting and renaming specific columns\n",
    "    TT_merged = TT[['firm', 'iso_o', 'iso_d', 'category', 'year', 'normalized_trade_value_pred', 'mean_ci_lower', 'mean_ci_upper']].copy()\n",
    "    TT_merged.rename(columns={\n",
    "        'normalized_trade_value_pred': 'trade_value_pred',\n",
    "        'mean_ci_lower': 'lb_value',\n",
    "        'mean_ci_upper': 'ub_value'\n",
    "    }, inplace=True)\n",
    "\n",
    "    return TT_merged\n",
    "\n",
    "\n",
    "final_results_ot_intervals = {}\n",
    "columns_to_sum = ['trade_value_pred', 'ub_value', 'lb_value']  # Ensure these columns exist in your DataFrame\n",
    "for year in range(2016, 2022):\n",
    "    final_results_ot_intervals[year] = calculate_confidence_interval(final_results_ot[year])\n",
    "    df = final_results_ot_intervals[year].copy()\n",
    "    df = df[df['iso_o'] != df['iso_d']]\n",
    "    df.to_csv(f\"firm_trade_ot_intervals_{year}.csv\", index=False)\n",
    "    df_aggregated = df.groupby(['iso_o', 'iso_d','year','category'], observed=False)[columns_to_sum].sum().reset_index()\n",
    "    df_aggregated.to_csv(f\"country_trade_ot_intervals_{year}.csv\", index=False)\n",
    "    print(year)\n",
    "\n",
    "final_results_hq_intervals = {}\n",
    "for year in range(2016, 2022):\n",
    "    final_results_hq_intervals[year] = calculate_confidence_interval(final_results_hq[year])   \n",
    "    df = final_results_hq_intervals[year].copy()\n",
    "    df = df[df['iso_o'] != df['iso_d']]\n",
    "    df.to_csv(f\"firm_trade_hq_intervals_{year}.csv\", index=False)\n",
    "    df_aggregated = df.groupby(['iso_o', 'iso_d','year','category'], observed=False)[columns_to_sum].sum().reset_index()\n",
    "    df_aggregated.to_csv(f\"country_trade_hq_intervals_{year}.csv\", index=False)\n",
    "    print(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXPORT THE DATA AS FOUR LARGE CSV FILES\n",
    "\n",
    "# Your code modification starts here\n",
    "columns_to_sum = ['trade_value_pred', 'ub_value', 'lb_value']\n",
    "\n",
    "# Initialize empty DataFrames for concatenation\n",
    "all_firm_trade_ot = pd.DataFrame()\n",
    "all_country_trade_ot = pd.DataFrame()\n",
    "all_firm_trade_hq = pd.DataFrame()\n",
    "all_country_trade_hq = pd.DataFrame()\n",
    "\n",
    "for year in range(2016, 2022):\n",
    "    # OT Data\n",
    "    df_ot = calculate_confidence_interval(final_results_ot[year])\n",
    "    df_ot = df_ot[df_ot['iso_o'] != df_ot['iso_d']]\n",
    "    df_ot = df_ot[df_ot['trade_value_pred'] != 0]\n",
    "    all_firm_trade_ot = pd.concat([all_firm_trade_ot, df_ot], ignore_index=True)\n",
    "    \n",
    "    # Aggregate OT data\n",
    "    df_aggregated_ot = df_ot.groupby(['iso_o', 'iso_d', 'year', 'category'], observed=False)[columns_to_sum].sum().reset_index()\n",
    "    df_aggregated_ot = df_aggregated_ot[df_aggregated_ot['trade_value_pred'] != 0]\n",
    "    all_country_trade_ot = pd.concat([all_country_trade_ot, df_aggregated_ot], ignore_index=True)\n",
    "    \n",
    "    # HQ Data\n",
    "    df_hq = calculate_confidence_interval(final_results_hq[year])\n",
    "    df_hq = df_hq[df_hq['iso_o'] != df_hq['iso_d']]\n",
    "    df_hq = df_hq[df_hq['trade_value_pred'] != 0]\n",
    "    all_firm_trade_hq = pd.concat([all_firm_trade_hq, df_hq], ignore_index=True)\n",
    "    \n",
    "    # Aggregate HQ data\n",
    "    df_aggregated_hq = df_hq.groupby(['iso_o', 'iso_d', 'year', 'category'], observed=False)[columns_to_sum].sum().reset_index()\n",
    "    df_aggregated_hq = df_aggregated_hq[df_aggregated_hq['trade_value_pred'] != 0]\n",
    "    all_country_trade_hq = pd.concat([all_country_trade_hq, df_aggregated_hq], ignore_index=True)\n",
    "\n",
    "# Export to CSV\n",
    "all_firm_trade_ot.to_csv(\"all_firm_trade_ot_intervals.csv\", index=False)\n",
    "all_country_trade_ot.to_csv(\"all_country_trade_ot_intervals.csv\", index=False)\n",
    "all_firm_trade_hq.to_csv(\"all_firm_trade_hq_intervals.csv\", index=False)\n",
    "all_country_trade_hq.to_csv(\"all_country_trade_hq_intervals.csv\", index=False)\n",
    "\n",
    "print(\"CSV files have been exported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eada22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BUBBLE CHART DATA\n",
    "\n",
    "# List of countries in EU-27\n",
    "eu_countries = ['AUT', 'BEL', 'BGR', 'HRV', 'CYP', 'CZE', 'DNK', 'EST', 'FIN', 'FRA', 'DEU', 'GRC', 'HUN', 'IRL', 'ITA', 'LVA', 'LTU', 'LUX', 'MLT', 'NLD', 'POL', 'PRT', 'ROU', 'SVK', 'SVN', 'ESP', 'SWE']\n",
    "\n",
    "# Function to process DataFrame\n",
    "def process_data(df):\n",
    "    # Remove rows where iso_o is equal to iso_d\n",
    "    df = df[df['iso_o'] != df['iso_d']].copy()\n",
    "\n",
    "    # Replace iso_o with 'EU-27' for EU countries\n",
    "    df['iso_o'] = df['iso_o'].apply(lambda x: 'EU-27' if x in eu_countries else x)\n",
    "    \n",
    "    pivot_df = df.groupby(['firm', 'iso_o'])['trade_value_pred'].sum().reset_index()\n",
    "    \n",
    "    filtered_aggregated_df = pivot_df[pivot_df['trade_value_pred'] > 0]\n",
    "\n",
    "    # Aggregate trade_value_pred by firm and iso_o\n",
    "    return filtered_aggregated_df\n",
    "\n",
    "# Processing final_results_ot_intervals and final_results_hq_intervals for the year 2021\n",
    "aggregated_ot_2021 = process_data(final_results_ot_intervals[2021])\n",
    "aggregated_hq_2021 = process_data(final_results_hq_intervals[2021])\n",
    "\n",
    "# Exporting to Excel\n",
    "with pd.ExcelWriter('aggregated_trade_values_2021.xlsx') as writer:\n",
    "    aggregated_ot_2021.to_excel(writer, sheet_name='OT_2021', index=False)\n",
    "    aggregated_hq_2021.to_excel(writer, sheet_name='HQ_2021', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea12cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXPORT THE LIST OF FIRMS AND APPS/GAMES\n",
    "\n",
    "# Filter the DataFrame for rows where category is 'apps' or 'games'\n",
    "filtered_df = TT_reduced[TT_reduced['category'].isin(['apps', 'games'])]\n",
    "\n",
    "# Finding unique entries in the columns 'firm', 'category', 'iso_o'\n",
    "unique_entries = filtered_df[['firm', 'category', 'iso_o']].drop_duplicates()\n",
    "\n",
    "# Exporting to CSV\n",
    "unique_entries.to_csv('apps_unique_category_entries.csv', index=False)\n",
    "\n",
    "# Finding unique entries in the columns 'firm', 'category', 'iso_o'\n",
    "unique_entries = filtered_df[['firm', 'iso_o']].drop_duplicates()\n",
    "\n",
    "# Exporting to CSV\n",
    "unique_entries.to_csv('apps_unique_entries.csv', index=False)\n",
    "\n",
    "\n",
    "# Finding unique rows for specified columns\n",
    "unique_rows = firm_subsidiary_revenues[['firm', 'iso_o', 'category', 'parent']].drop_duplicates()\n",
    "\n",
    "# Exporting to CSV\n",
    "output_file = 'firms_unique_category_entries.csv'\n",
    "unique_rows.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "# Finding unique rows for specified columns\n",
    "unique_rows = firm_subsidiary_revenues[['firm', 'iso_o', 'parent']].drop_duplicates()\n",
    "\n",
    "# Exporting to CSV\n",
    "output_file = 'firms_unique_entries.csv'\n",
    "unique_rows.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
